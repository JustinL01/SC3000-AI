{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run:\n",
    "1. conda env create -f environment.yml\n",
    "2. conda activate sc3000_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install xvfb, python-opengl, ffmpeg and cmake with conda\n",
    "# !conda install -c conda-forge xvfbwrapper pyopengl ffmpeg cmake\n",
    "# !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "# !pip install gym[classic_control]\n",
    "# !pip install --upgrade setuptools 2>&1\n",
    "# !pip install ez_setup > /dev/null 2>&1\n",
    "# !pip install tensorflow\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Dependencies and Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import logger as gymlogger\n",
    "from gym.wrappers import RecordVideo\n",
    "gymlogger.set_level(40) #error only\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial observation: (array([-0.00014852,  0.04895825, -0.04297311, -0.04529496], dtype=float32), {})\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "obs = env.reset()\n",
    "print(\"Initial observation:\", obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Task 1** : Development of an RL agent\n",
    "> Development of an RL agent. Demonstrate the correctness of the implementation by sampling a random state from the cart pole environment, inputting to the agent, and outputting a chosen action. Print the values of the state and chosen action in Jupyter notebook.\n",
    "\n",
    "\n",
    "## Approach\n",
    "We utilised Q-Learning via Temporal Difference (Epsilon Soft/Greedy function with decaying epsilon), optimised using our own method of hyperparameter sampling and analysis.\n",
    "\n",
    "\n",
    "We analysed the set of hyperparameters used in our samples along with the rewards produced by them, to derive insights on the values of each parameter we should use in training the RL agent.\n",
    "\n",
    "## How It Improves Our Agent\n",
    "Approaching this problem for the first time, we do not know what a good set of hyperparameters are. By running 10 random sets of hyperparameters, we can see what hyperparameters do badly, and why some hyperparameters are better.\n",
    "\n",
    "In doing so, we can also conclude findings for this environment - for example, by increasing gamma closer to 1, we manage to achieve higher rewards, as it is generally favourable to consider future rewards in the context of the cartpole environment. (the output is analysed below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cartpole Environment**\n",
    "Step 1: We utilise OpenAI's Gym library to load the Cartpole-v1 environment, with all the rewards and conditions in place\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: We check the action and observation space of it. The output \"2\" shows that we have two valid discrete actions, 0 and 1 (left & right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "actionNumber = env.action_space.n\n",
    "print(actionNumber)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Initialisation of Global Variables**\n",
    "Step 3: We define our hyper-parameter search space as we plan to use\n",
    "random search and retrieve an optimised set of hyper-parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_space = {\n",
    "    'gamma': np.linspace(0.9, 1, 10),\n",
    "    'epsilonParameter': np.linspace(7000, 8000, 5),\n",
    "    'noOfEpisodesForRandom': np.linspace(300, 600, 5, dtype=int),\n",
    "    'numberOfBins': np.linspace(25, 30, 5, dtype=int),\n",
    "    'epsilon': np.linspace(0.1, 0.4, 5),\n",
    "    'alpha': np.linspace(0.1, 0.4, 5)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some explanation for the hyperparameters:\n",
    "\n",
    "1) gamma:\n",
    "It represents the discount factor for future rewards, determining the importance of future rewards in the agent's decision-making process. A value closer to 1 indicates that the agent values future rewards highly, while a value closer to 0 indicates a preference for immediate rewards.\n",
    "\n",
    "2) epsilonParameter:\n",
    "It controls the balance between exploring new actions and exploiting known actions that have yielded favorable outcomes in the past. A higher epsilon value encourages more exploration, while a lower value favors exploitation of the current best action.\n",
    "\n",
    "3) noOfEpisodesForRandom:\n",
    "It specifies the number of episodes dedicated to random exploration during the training phase of a reinforcement learning algorithm.\n",
    "\n",
    "4) numberOfBins:\n",
    "It is used to discretize a continuous space into discrete bins, influencing the granularity of the state space representation, affecting the agent's ability to distinguish between different states.\n",
    "\n",
    "5) epsilon:\n",
    "It affects the exploration, representing the probability of taking a random action instead of following the current policy. Unlike epsilonParameter that controls the overall exploration-exploitation trade-off, epsilon determines the probability of taking a random action.\n",
    "\n",
    "6) alpha:\n",
    "It denotes the learning rate, which determines the extent to which newly acquired information overrides old information during the updating of the agent's knowledge. A higher alpha value indicates a greater reliance on recent experiences, potentially leading to faster adaptation to changes in the environment. However, a very high alpha can also cause the agent to forget valuable past experiences\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc3000_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
